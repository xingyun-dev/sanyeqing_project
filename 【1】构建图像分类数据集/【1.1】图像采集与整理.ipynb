{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像采集与整理\n",
    "\n",
    "\n",
    "三叶青块根图片\n",
    "\n",
    "从网络上爬取的其它图片\n",
    "\n",
    "图片数据集的处理\n",
    "\n",
    "图片数据集的混合\n",
    "\n",
    "划分训练集与测试集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三叶青块根图片采集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三叶青块根图片主要是通过自己拍照来采集的\n",
    "\n",
    ">具体信息，请参考【8】相关补充——三叶青实验记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从网络上爬取的其它图片\n",
    "\n",
    ">将这些照片充当未知类别，另外还将本地相册中的照片也作为未知类别。（有点建议：或许我要挑选一些与三叶青块根相似的其它药材作为未知类别，这样更能区分出三叶青块根）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新建文件夹：dataset/黄瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:12,  1.42s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/南瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:14,  1.46s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/冬瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:15,  1.48s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/木瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:12,  1.43s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/苦瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:12,  1.41s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/丝瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:14,  1.46s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/窝瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:11,  1.40s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/甜瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:11,  1.39s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/香瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:07,  1.32s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/白兰瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:08,  1.34s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/黄金瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:07,  1.32s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/西葫芦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:12,  1.41s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/人参果\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [00:43<00:28,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新建文件夹：dataset/羊角蜜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:05,  1.29s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/佛手瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:06,  1.30s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n",
      "新建文件夹：dataset/伊丽莎白瓜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:05,  1.29s/it]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 张图像爬取完毕\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "from tqdm import tqdm  # 进度条库\n",
    "import os\n",
    "#https请求参数\n",
    "cookies = {\n",
    "'BDqhfp': '%E7%8B%97%E7%8B%97%26%26NaN-1undefined%26%2618880%26%2621',\n",
    "'BIDUPSID': '06338E0BE23C6ADB52165ACEB972355B',\n",
    "'PSTM': '1646905430',\n",
    "'BAIDUID': '104BD58A7C408DABABCAC9E0A1B184B4:FG=1',\n",
    "'BDORZ': 'B490B5EBF6F3CD402E515D22BCDA1598',\n",
    "'H_PS_PSSID': '35836_35105_31254_36024_36005_34584_36142_36120_36032_35993_35984_35319_26350_35723_22160_36061',\n",
    "'BDSFRCVID': '8--OJexroG0xMovDbuOS5T78igKKHJQTDYLtOwXPsp3LGJLVgaSTEG0PtjcEHMA-2ZlgogKK02OTH6KF_2uxOjjg8UtVJeC6EG0Ptf8g0M5',\n",
    "'H_BDCLCKID_SF': 'tJPqoKtbtDI3fP36qR3KhPt8Kpby2D62aKDs2nopBhcqEIL4QTQM5p5yQ2c7LUvtynT2KJnz3Po8MUbSj4QoDjFjXJ7RJRJbK6vwKJ5s5h5nhMJSb67JDMP0-4F8exry523ioIovQpn0MhQ3DRoWXPIqbN7P-p5Z5mAqKl0MLPbtbb0xXj_0D6bBjHujtT_s2TTKLPK8fCnBDP59MDTjhPrMypomWMT-0bFH_-5L-l5js56SbU5hW5LSQxQ3QhLDQNn7_JjOX-0bVIj6Wl_-etP3yarQhxQxtNRdXInjtpvhHR38MpbobUPUDa59LUvEJgcdot5yBbc8eIna5hjkbfJBQttjQn3hfIkj0DKLtD8bMC-RDjt35n-Wqxobbtof-KOhLTrJaDkWsx7Oy4oTj6DD5lrG0P6RHmb8ht59JROPSU7mhqb_3MvB-fnEbf7r-2TP_R6GBPQtqMbIQft20-DIeMtjBMJaJRCqWR7jWhk2hl72ybCMQlRX5q79atTMfNTJ-qcH0KQpsIJM5-DWbT8EjHCet5DJJn4j_Dv5b-0aKRcY-tT5M-Lf5eT22-usy6Qd2hcH0KLKDh6gb4PhQKuZ5qutLTb4QTbqWKJcKfb1MRjvMPnF-tKZDb-JXtr92nuDal5TtUthSDnTDMRhXfIL04nyKMnitnr9-pnLJpQrh459XP68bTkA5bjZKxtq3mkjbPbDfn02eCKuj6tWj6j0DNRabK6aKC5bL6rJabC3b5CzXU6q2bDeQN3OW4Rq3Irt2M8aQI0WjJ3oyU7k0q0vWtvJWbbvLT7johRTWqR4enjb3MonDh83Mxb4BUrCHRrzWn3O5hvvhKoO3MA-yUKmDloOW-TB5bbPLUQF5l8-sq0x0bOte-bQXH_E5bj2qRCqVIKa3f',\n",
    "'BDSFRCVID_BFESS': '8--OJexroG0xMovDbuOS5T78igKKHJQTDYLtOwXPsp3LGJLVgaSTEG0PtjcEHMA-2ZlgogKK02OTH6KF_2uxOjjg8UtVJeC6EG0Ptf8g0M5',\n",
    "'H_BDCLCKID_SF_BFESS': 'tJPqoKtbtDI3fP36qR3KhPt8Kpby2D62aKDs2nopBhcqEIL4QTQM5p5yQ2c7LUvtynT2KJnz3Po8MUbSj4QoDjFjXJ7RJRJbK6vwKJ5s5h5nhMJSb67JDMP0-4F8exry523ioIovQpn0MhQ3DRoWXPIqbN7P-p5Z5mAqKl0MLPbtbb0xXj_0D6bBjHujtT_s2TTKLPK8fCnBDP59MDTjhPrMypomWMT-0bFH_-5L-l5js56SbU5hW5LSQxQ3QhLDQNn7_JjOX-0bVIj6Wl_-etP3yarQhxQxtNRdXInjtpvhHR38MpbobUPUDa59LUvEJgcdot5yBbc8eIna5hjkbfJBQttjQn3hfIkj0DKLtD8bMC-RDjt35n-Wqxobbtof-KOhLTrJaDkWsx7Oy4oTj6DD5lrG0P6RHmb8ht59JROPSU7mhqb_3MvB-fnEbf7r-2TP_R6GBPQtqMbIQft20-DIeMtjBMJaJRCqWR7jWhk2hl72ybCMQlRX5q79atTMfNTJ-qcH0KQpsIJM5-DWbT8EjHCet5DJJn4j_Dv5b-0aKRcY-tT5M-Lf5eT22-usy6Qd2hcH0KLKDh6gb4PhQKuZ5qutLTb4QTbqWKJcKfb1MRjvMPnF-tKZDb-JXtr92nuDal5TtUthSDnTDMRhXfIL04nyKMnitnr9-pnLJpQrh459XP68bTkA5bjZKxtq3mkjbPbDfn02eCKuj6tWj6j0DNRabK6aKC5bL6rJabC3b5CzXU6q2bDeQN3OW4Rq3Irt2M8aQI0WjJ3oyU7k0q0vWtvJWbbvLT7johRTWqR4enjb3MonDh83Mxb4BUrCHRrzWn3O5hvvhKoO3MA-yUKmDloOW-TB5bbPLUQF5l8-sq0x0bOte-bQXH_E5bj2qRCqVIKa3f',\n",
    "'indexPageSugList': '%5B%22%E7%8B%97%E7%8B%97%22%5D',\n",
    "'cleanHistoryStatus': '0',\n",
    "'BAIDUID_BFESS': '104BD58A7C408DABABCAC9E0A1B184B4:FG=1',\n",
    "'BDRCVFR[dG2JNJb_ajR]': 'mk3SLVN4HKm',\n",
    "'BDRCVFR[-pGxjrCMryR]': 'mk3SLVN4HKm',\n",
    "'ab_sr': '1.0.1_Y2YxZDkwMWZkMmY2MzA4MGU0OTNhMzVlNTcwMmM2MWE4YWU4OTc1ZjZmZDM2N2RjYmVkMzFiY2NjNWM4Nzk4NzBlZTliYWU0ZTAyODkzNDA3YzNiMTVjMTllMzQ0MGJlZjAwYzk5MDdjNWM0MzJmMDdhOWNhYTZhMjIwODc5MDMxN2QyMmE1YTFmN2QyY2M1M2VmZDkzMjMyOThiYmNhZA==',\n",
    "'delPer': '0',\n",
    "'PSINO': '2',\n",
    "'BA_HECTOR': '8h24a024042g05alup1h3g0aq0q',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "'Connection': 'keep-alive',\n",
    "'sec-ch-ua': '\" Not;A Brand\";v=\"99\", \"Google Chrome\";v=\"97\", \"Chromium\";v=\"97\"',\n",
    "'Accept': 'text/plain, */*; q=0.01',\n",
    "'X-Requested-With': 'XMLHttpRequest',\n",
    "'sec-ch-ua-mobile': '?0',\n",
    "'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36',\n",
    "'sec-ch-ua-platform': '\"macOS\"',\n",
    "'Sec-Fetch-Site': 'same-origin',\n",
    "'Sec-Fetch-Mode': 'cors',\n",
    "'Sec-Fetch-Dest': 'empty',\n",
    "'Referer': 'https://image.baidu.com/search/index?tn=baiduimage&ipn=r&ct=201326592&cl=2&lm=-1&st=-1&fm=result&fr=&sf=1&fmq=1647837998851_R&pv=&ic=&nc=1&z=&hd=&latest=&copyright=&se=1&showtab=0&fb=0&width=&height=&face=0&istype=2&dyTabStr=MCwzLDIsNiwxLDUsNCw4LDcsOQ%3D%3D&ie=utf-8&sid=&word=%E7%8B%97%E7%8B%97',\n",
    "'Accept-Language': 'zh-CN,zh;q=0.9',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#爬取函数\n",
    "def craw_single_class(keyword, DOWNLOAD_NUM = 200):\n",
    "    if os.path.exists('dataset/'+keyword):\n",
    "        print('文件夹 dataset/{} 已存在，之后直接将爬取到的图片保存至该文件夹中'.format(keyword))\n",
    "    else:\n",
    "        os.makedirs('dataset/{}'.format(keyword))\n",
    "        print('新建文件夹：dataset/{}'.format(keyword))\n",
    "    count = 1\n",
    "    \n",
    "    with tqdm(total=DOWNLOAD_NUM, position=0, leave=True) as pbar:\n",
    "        \n",
    "        # 爬取第几张\n",
    "        num = 0\n",
    "\n",
    "        # 是否继续爬取\n",
    "        FLAG = True\n",
    "\n",
    "        while FLAG:\n",
    "\n",
    "            page = 30 * count\n",
    "\n",
    "            params = (\n",
    "                ('tn', 'resultjson_com'),\n",
    "                ('logid', '12508239107856075440'),\n",
    "                ('ipn', 'rj'),\n",
    "                ('ct', '201326592'),\n",
    "                ('is', ''),\n",
    "                ('fp', 'result'),\n",
    "                ('fr', ''),\n",
    "                ('word', f'{keyword}'),\n",
    "                ('queryWord', f'{keyword}'),\n",
    "                ('cl', '2'),\n",
    "                ('lm', '-1'),\n",
    "                ('ie', 'utf-8'),\n",
    "                ('oe', 'utf-8'),\n",
    "                ('adpicid', ''),\n",
    "                ('st', '-1'),\n",
    "                ('z', ''),\n",
    "                ('ic', ''),\n",
    "                ('hd', ''),\n",
    "                ('latest', ''),\n",
    "                ('copyright', ''),\n",
    "                ('s', ''),\n",
    "                ('se', ''),\n",
    "                ('tab', ''),\n",
    "                ('width', ''),\n",
    "                ('height', ''),\n",
    "                ('face', '0'),\n",
    "                ('istype', '2'),\n",
    "                ('qc', ''),\n",
    "                ('nc', '1'),\n",
    "                ('expermode', ''),\n",
    "                ('nojc', ''),\n",
    "                ('isAsync', ''),\n",
    "                ('pn', f'{page}'),\n",
    "                ('rn', '30'),\n",
    "                ('gsm', '1e'),\n",
    "                ('1647838001666', ''),\n",
    "            )\n",
    "\n",
    "            response = requests.get('https://image.baidu.com/search/acjson', headers=headers, params=params, cookies=cookies)\n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    json_data = response.json().get(\"data\")\n",
    "\n",
    "                    if json_data:\n",
    "                        for x in json_data:\n",
    "                            type = x.get(\"type\")\n",
    "                            if type not in [\"gif\"]:\n",
    "                                img = x.get(\"thumbURL\")\n",
    "                                fromPageTitleEnc = x.get(\"fromPageTitleEnc\")\n",
    "                                try:\n",
    "                                    resp = requests.get(url=img, verify=False)\n",
    "                                    time.sleep(1)\n",
    "                                    # print(f\"链接 {img}\")\n",
    "\n",
    "                                    # 保存文件名\n",
    "                                    # file_save_path = f'dataset/{keyword}/{num}-{fromPageTitleEnc}.{type}'\n",
    "                                    file_save_path = f'dataset/{keyword}/{num}.{type}'\n",
    "                                    with open(file_save_path, 'wb') as f:                                    \n",
    "                                        f.write(resp.content)\n",
    "                                        f.flush()\n",
    "                                        # print('第 {} 张图像 {} 爬取完成'.format(num, fromPageTitleEnc))\n",
    "                                        num += 1\n",
    "                                        pbar.update(1) # 进度条更新\n",
    "\n",
    "                                    # 爬取数量达到要求\n",
    "                                    if num > DOWNLOAD_NUM:\n",
    "                                        FLAG = False\n",
    "                                        print('{} 张图像爬取完毕'.format(num))\n",
    "                                        break\n",
    "\n",
    "                                except Exception:\n",
    "                                    pass\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            count += 1\n",
    "\n",
    "\n",
    "class_list = ['黄瓜','南瓜','冬瓜','木瓜','苦瓜','丝瓜','窝瓜','甜瓜','香瓜','白兰瓜','黄金瓜','西葫芦','人参果','羊角蜜','佛手瓜','伊丽莎白瓜']\n",
    "for each in class_list:\n",
    "    craw_single_class(each, DOWNLOAD_NUM = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些参考类别关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 苹果\n",
    "'苹果 水果','青苹果'\n",
    "# 常见水果\n",
    "'菠萝','榴莲','椰子','香蕉','梨','芒果'\n",
    "# 西红柿类\n",
    "'圣女果','西红柿'\n",
    "# 桔橙类\n",
    "'砂糖橘','脐橙','金桔','柠檬','西柚','血橙','芦柑','青柠','沃柑','粑粑柑','橘子','柚子'\n",
    "# 桃类\n",
    "'猕猴桃','油桃','水蜜桃','蟠桃','杨桃','黄桃'\n",
    "# 樱桃类\n",
    "'樱桃','智利 车厘子'\n",
    "# 火龙果类\n",
    "'白心火龙果','红心火龙果'\n",
    "# 葡萄类\n",
    "'马奶提子','红提'\n",
    "# 萝卜类\n",
    "'胡萝卜','白萝卜'\n",
    "# 莓类\n",
    "'桑葚','蔓越莓','蓝莓','草莓','树莓','菠萝莓''黑莓 水果'\n",
    "# 其它类\n",
    "'山楂','桂圆','杨梅','西梅','沙果','枣','荔枝','腰果','无花果','沙棘','羊奶果','百香果','黄金百香果','甘蔗','菠萝蜜','酸角','蛇皮果','人参果','红芭乐','白芭乐','牛油果','莲雾','山竹','杏','李子','柿子','枇杷','香橼','毛丹','石榴'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图片数据集的处理\n",
    "\n",
    "前面获得了三叶青块根照片和其它任意图片的数据集。\n",
    "但是数据集的格式、图片大小并未统一。\n",
    "\n",
    ">包括：\n",
    ">1. 复制原始图片，生成图片的标签文件\n",
    ">2. 根据labels.csv文件整理为labels_2.csv或labels_5.csv或lebels_未知.csv\n",
    ">3. 图片压缩为统一尺寸\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "#复制原始图片，生成图片的标签文件\n",
    "def photo_copy(source_folder, destination_folder, by_region):\n",
    "    \"\"\"\n",
    "    :param source_folder:\n",
    "    :param destination_folder:\n",
    "    :param by_region:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    listdir = os.listdir(source_folder)\n",
    "    df = pd.DataFrame(columns=['ID', 'labels'])\n",
    "    counter = 0\n",
    "\n",
    "    for i in listdir:\n",
    "        print(i)\n",
    "        listdir_i = os.listdir(os.path.join(source_folder, i))\n",
    "        if listdir_i:\n",
    "            for j, file in enumerate(listdir_i):\n",
    "                print(j, file)\n",
    "                if by_region == \"二级目录-按地区名称\":\n",
    "                    region = file  # 获取地区名称\n",
    "                    listdir_i_j = os.listdir(os.path.join(source_folder, i, file))\n",
    "                    for f in listdir_i_j:\n",
    "                        source_file = os.path.join(source_folder, i, file, f)\n",
    "                        destination_file_name = str(counter) + '.png'\n",
    "                        counter += 1\n",
    "                        destination_file = os.path.join(destination_folder, destination_file_name)\n",
    "                        os.makedirs(os.path.dirname(destination_file), exist_ok=True)\n",
    "                        shutil.copy2(source_file, destination_file)\n",
    "                        new_row = pd.DataFrame([[counter - 1, region]], columns=['ID', 'labels'])\n",
    "                        df = pd.concat([df, new_row], ignore_index=True)\n",
    "                elif by_region == \"二级目录-按省份名称\":\n",
    "                    listdir_i_j = os.listdir(os.path.join(source_folder, i, file))\n",
    "                    for f in listdir_i_j:\n",
    "                        source_file = os.path.join(source_folder, i, file, f)\n",
    "                        destination_file_name = str(counter) + '.png'\n",
    "                        counter += 1\n",
    "                        destination_file = os.path.join(destination_folder, destination_file_name)\n",
    "                        os.makedirs(os.path.dirname(destination_file), exist_ok=True)\n",
    "                        shutil.copy2(source_file, destination_file)\n",
    "                        new_row = pd.DataFrame([[counter - 1, i]], columns=['ID', 'labels'])\n",
    "                        df = pd.concat([df, new_row], ignore_index=True)\n",
    "                else:\n",
    "                    source_file = os.path.join(source_folder, i, file)\n",
    "                    destination_file_name = str(counter) + '.png'\n",
    "                    counter += 1\n",
    "                    destination_file = os.path.join(destination_folder, destination_file_name)\n",
    "                    os.makedirs(os.path.dirname(destination_file), exist_ok=True)\n",
    "                    shutil.copy2(source_file, destination_file)\n",
    "                    new_row = pd.DataFrame([[counter - 1, i]], columns=['ID', 'labels'])\n",
    "                    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "        print(), print()\n",
    "        print(\"*********************************************\")\n",
    "    df.to_csv(r'D:\\labels.csv', index=False)\n",
    "#\n",
    "#\n",
    "# if __name__ == '__main__':\n",
    "#     photo_copy(r'D:\\weizhi-photo\\dataset', r'D:\\train_weizhi', by_region=False)\n",
    "#\n",
    "\n",
    "# 根据labels.csv文件整理为labels_2.csv或labels_5.csv或lebels_未知.csv\n",
    "def labels_to_x(source_file, destination_file, is_lei):\n",
    "    df = pd.read_csv(source_file)\n",
    "    if is_lei == 2:\n",
    "        df['labels'] = df['labels'].apply(lambda x: x[:2] if x[:2] == 'ZJ' else 'not_ZJ')\n",
    "    elif is_lei == 5:\n",
    "        df['labels'] = df['labels'].apply(lambda x: x[:2])\n",
    "    elif is_lei == \"未知\":\n",
    "        df['labels'] = df['labels'].apply(lambda x: '未知')\n",
    "    df.to_csv(destination_file, index=False)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     labels_to_x(\"D:\\labels.csv\", \"D:\\labels_weizhi.csv\",is_lei=\"未知\")\n",
    "#     print(\"===================\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#图片压缩为统一尺寸\n",
    "def resize_images(source_dir, target_dir, max_width, max_height, target_size=None):\n",
    "    \"\"\"\n",
    "    Resize images in a directory and save them to another directory.\n",
    "\n",
    "    :param source_dir: The directory of the original images.\n",
    "    :param target_dir: The directory to save the resized images.\n",
    "    :param max_width: The maximum width of the resized images.\n",
    "    :param max_height: The maximum height of the resized images.\n",
    "    :param target_size: The target size of the resized images. If specified, the images will be resized to this size.\n",
    "    \"\"\"\n",
    "    # 确保目标文件夹存在\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    # 遍历源文件夹中的所有文件\n",
    "    for filename in os.listdir(source_dir):\n",
    "        # 检查文件是否是图片（你可以根据需要添加其他格式）\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            # 构建源文件和目标文件的完整路径\n",
    "            source_path = os.path.join(source_dir, filename)\n",
    "            target_path = os.path.join(target_dir, filename)\n",
    "\n",
    "            try:\n",
    "                # 打开图片\n",
    "                with Image.open(source_path) as img:\n",
    "                    if target_size:  # 强制压缩\n",
    "                        # 裁剪或缩放图片以适应目标尺寸\n",
    "                        img_resized = img.resize(target_size)\n",
    "                        img_resized.save(target_path)\n",
    "                    else:\n",
    "                        # 获取原始图片的宽度和高度\n",
    "                        width, height = img.size\n",
    "                        # 计算新的尺寸，保持长宽比\n",
    "                        if width > height:\n",
    "                            new_width = max_width\n",
    "                            new_height = int(height * (max_width / width))\n",
    "                        else:\n",
    "                            new_height = max_height\n",
    "                            new_width = int(width * (max_height / height))\n",
    "                            # 调整图片大小并保持长宽比\n",
    "                        img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "                        # 保存调整后的图片到目标文件夹\n",
    "                        img.save(target_path)\n",
    "\n",
    "                    # 打印进度信息（可选）\n",
    "                print(f\"Resized and copied {filename} to {target_dir}\")\n",
    "            except Exception as e:\n",
    "                # 打印错误信息并继续处理其他文件\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    print(\"Resizing and copying completed.\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # 原始图片文件夹路径\n",
    "#     source_dir = r\"D:\\train_weizhi\"\n",
    "#     # 目标文件夹路径，调整大小后的图片将被复制到这里\n",
    "#     target_dir = r\"D:\\train_weizhi_yasuo\"\n",
    "#     # 指定的图片宽度和高度(保持原始图片的长宽比进行压缩）\n",
    "#     max_width = 400\n",
    "#     max_height = 400\n",
    "#     target_size = None\n",
    "#     # target_size = (400, 400)  # 强制压缩为640×640\n",
    "#     resize_images(source_dir, target_dir, max_width,max_height, target_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图片数据集的混合\n",
    "\n",
    ">将三叶青的块根照片与未知照片混合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将三叶青的块根照片与未知照片混合\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "def hunhe_sanyeqing(source_folder_bai, source_folder_fei, target_folder, labels_csv, labels_feibai_csv):\n",
    "    # 读取标签文件\n",
    "    labels_bai = pd.read_csv(labels_csv,encoding='utf-8')\n",
    "    labels_fei = pd.read_csv(labels_feibai_csv,encoding=\"utf-8\")\n",
    "\n",
    "    # 确保目标文件夹存在\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    # 准备一个列表来存储图片的ID和标签\n",
    "    labels = []\n",
    "\n",
    "    # 遍历源文件夹及其所有子文件夹中的文件\n",
    "    for i, bai_file in enumerate(labels_bai['ID']):\n",
    "        for dirpath, dirnames, filenames in os.walk(source_folder_bai):\n",
    "            if f\"{bai_file}.png\" in filenames:\n",
    "                # 复制并重命名图片文件\n",
    "                shutil.copy(os.path.join(dirpath, f\"{bai_file}.png\"), os.path.join(target_folder, f\"{i}.png\"))\n",
    "                # 记录图片的ID和标签\n",
    "                labels.append((i, labels_bai.loc[i, 'labels']))\n",
    "                break\n",
    "\n",
    "    for i, fei_file in enumerate(labels_fei['ID']):\n",
    "        for dirpath, dirnames, filenames in os.walk(source_folder_fei):\n",
    "            if f\"{fei_file}.png\" in filenames:\n",
    "                # 复制并重命名图片文件\n",
    "                shutil.copy(os.path.join(dirpath, f\"{fei_file}.png\"), os.path.join(target_folder, f\"{i+len(labels_bai)}.png\"))\n",
    "                # 记录图片的ID和标签\n",
    "                labels.append((i+len(labels_bai), labels_fei.loc[i, 'labels']))\n",
    "                break\n",
    "\n",
    "    # 将图片的ID和标签保存到CSV文件中\n",
    "    df = pd.DataFrame(labels, columns=[\"ID\", \"labels\"])\n",
    "    df.to_csv(os.path.join(target_folder, \"labels_hun.csv\"), index=False)\n",
    "\n",
    "\n",
    "\n",
    "#形成改变后的labels.csv文件\n",
    "def labels_to_custom(source_file, destination_file):\n",
    "    # 创建一个字典来映射原始标签和新标签\n",
    "    # label_dict = {\n",
    "    #     \"GXBS\": \"广西省\",\n",
    "    #     \"GXGL\": \"广西省\",\n",
    "    #     \"GXYL\": \"广西省\",\n",
    "    #     \"GZBJ\": \"贵州省\",\n",
    "    #     \"GZQXN\": \"贵州省\",\n",
    "    #     \"SXXA\": \"陕西省\",\n",
    "    #     'YNKM': '云南省',\n",
    "    #     \"YNXSBN\": \"云南省\",\n",
    "    #     \"ZJTZ\": \"浙江省\",\n",
    "    #     \"ZJWZ\": \"浙江省\",\n",
    "    #     '未知': '未知',  # 处理未知的标签\n",
    "    # }\n",
    "    # label_dict = {\n",
    "    #     \"GXBS\": \"广西百色\",\n",
    "    #     \"GXGL\": \"广西桂林\",\n",
    "    #     \"GXYL\": \"广西玉林\",\n",
    "    #     \"GZBJ\": \"贵州毕节\",\n",
    "    #     \"GZQXN\": \"贵州黔西南\",\n",
    "    #     \"SXXA\": \"陕西西安\",\n",
    "    #     'YNKM': '云南昆明',\n",
    "    #     \"YNXSBN\": \"云南西双版纳\",\n",
    "    #     \"ZJTZ\": \"浙江台州\",\n",
    "    #     \"ZJWZ\": \"浙江温州\",\n",
    "    #     '未知': '未知',  # 处理未知的标签\n",
    "    # }\n",
    "    label_dict = {\n",
    "        '浙江省':'浙江省',\n",
    "        \"广西省\":'非浙江',\n",
    "        \"贵州省\":'非浙江',\n",
    "        \"陕西省\":'非浙江',\n",
    "        '云南省':'非浙江',\n",
    "        '未知':'未知'\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.read_csv(source_file)\n",
    "    df['labels'] = df['labels'].apply(lambda x: label_dict.get(x, '未知'))\n",
    "    df.to_csv(destination_file, index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # hunhe_sanyeqing(source_folder_bai=r\"D:\\SanYeQing_Project\\linshi_caogao\\train_hun\",\n",
    "    #                 source_folder_fei=r\"D:\\SanYeQing_Project\\linshi_caogao\\weizhi\",\n",
    "    #                 target_folder=r\"D:\\SanYeQing_Project\\linshi_caogao\\train_hun_weizhi_11\",\n",
    "    #                 labels_csv=r\"D:\\SanYeQing_Project\\linshi_caogao\\labels_hun.csv\",\n",
    "    #                 labels_feibai_csv=r\"D:\\SanYeQing_Project\\linshi_caogao\\labels_hun_finally.csv\")\n",
    "\n",
    "    labels_to_custom(source_file=r\"D:\\SanYeQing_Project\\sanyeqing_hun_weizhi_finally\\labels_hun_finally.csv\",destination_file=r\"D:\\SanYeQing_Project\\sanyeqing_hun_weizhi_finally\\labels_hun_finally_2.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将测试集划分出来\n",
    "\n",
    "> 设置自定义的划分比例，这里为0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "def random_select_images(source_folder, target_folder, ratio):\n",
    "    # 遍历源文件夹及其子文件夹，收集图片文件\n",
    "    images = []\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "                images.append(os.path.join(root, file))\n",
    "\n",
    "    # 计算需要选择的图片数量\n",
    "    num_images = int(len(images) * ratio)\n",
    "\n",
    "    # 随机挑选指定数量的图片\n",
    "    selected_images = random.sample(images, num_images)\n",
    "\n",
    "    # 将挑选出的图片复制到目标文件夹，保留文件名\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "    for image in selected_images:\n",
    "        # 获取图片文件名（不包含路径）\n",
    "        filename = os.path.basename(image)\n",
    "        # 构造目标文件路径\n",
    "        target_path = os.path.join(target_folder, filename)\n",
    "        # 移动文件到目标路径\n",
    "        shutil.move(image, target_path)\n",
    "\n",
    "    return selected_images\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 使用示例\n",
    "    source_folder = r\"D:\\SanYeQing_Project\\linshi_caogao\\train_hun_weizhi_11\"  # 源文件夹路径\n",
    "    target_folder = r\"D:\\SanYeQing_Project\\linshi_caogao\\test_hun_weizhi_11\"  # 目标文件夹路径\n",
    "    ratio = 0.2  # 需要挑选的图片比例\n",
    "    random_select_images(source_folder, target_folder, ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计每个类别测试集的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class  test\n",
      "0   云南省   296\n",
      "1    未知   310\n",
      "2   浙江省   293\n",
      "3   贵州省   308\n",
      "4   陕西省   296\n",
      "5   广西省   339\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def count_images_by_label(test_folder, labels_df):\n",
    "    counts = {}\n",
    "    for root, dirs, files in os.walk(test_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "                # 获取文件名（不包含路径和后缀）\n",
    "                filename_without_ext = int(os.path.splitext(file)[0])\n",
    "                # print(filename_without_ext)\n",
    "                # print(labels_df.head())\n",
    "                # 在标签文件中查找对应的标签\n",
    "                label = labels_df.loc[labels_df['ID'] == filename_without_ext, 'labels'].values[0]\n",
    "                # 更新计数\n",
    "                if label in counts:\n",
    "                    counts[label] += 1\n",
    "                else:\n",
    "                    counts[label] = 1\n",
    "    # 创建一个 DataFrame 来存储结果\n",
    "    counts_df = pd.DataFrame(list(counts.items()), columns=['class', 'test'])\n",
    "    return counts_df\n",
    "\n",
    "# 使用示例\n",
    "test_folder = r\"D:\\SanYeQing_Project\\sanyeqing_hun_weizhi_finally\\test_hun_finally\"  # 测试集文件夹路径\n",
    "labels_path = r\"D:\\SanYeQing_Project\\sanyeqing_hun_weizhi_finally\\labels_hun.csv\"  # 标签文件路径\n",
    "labels_df = pd.read_csv(labels_path,index_col=None) #  假设labels.csv的格式是：ID,labels\n",
    "# print(labels_df.head())\n",
    "counts_df = count_images_by_label(test_folder, labels_df)\n",
    "print(counts_df)\n",
    "counts_df.to_csv(\"数据量统计.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
